# -*- coding: utf-8 -*-
"""EDA_Acadally.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Zo9lwpLU4JdUnu6EnuV2nz6Nu1JJ4-Dg

#Importing required libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""Since there are two different sheets of Data, therefore segregating both of them seperately using pandas read function directly into two different DataFrames"""

Attempt_df=pd.read_excel('Data Analyst Assignement Acadally.xlsx', sheet_name='Attempts Data')
Chapter_df=pd.read_excel('Data Analyst Assignement Acadally.xlsx', sheet_name='Chapter Data')

"""Initially we will have to go through the size and features of data so that we can check how much cleaning or pre-processing we have to do"""

Attempt_df.shape

Chapter_df.shape

print(Attempt_df.columns)
print(Chapter_df.columns)

Attempt_df.dtypes

Attempt_df['time'].sample(5)

Attempt_df.isnull().sum()

"""

```
No Nulls in this section, we can move to check duplicates which I think are likely to be present as lot of IDs are same in rows and then we will see how to handle them
```

"""

Attempt_df.duplicated().sum()

Attempt_df[Attempt_df.duplicated()].tail(10)

"""multiple columns have repeated values might show same school, syllabus or chapter, topic, section and quiz type but have variable qid, question_status still need to find completely duplicated rows to see if there are any to eliminate"""

Attempt_df.duplicated(keep=False).sum()

Attempt_df[Attempt_df.duplicated(keep=False)].sort_values(['user_id', 'qid', 'time']).head(10)

"""Time is also same and might be duplicate entries which are supposed to be clean as complete row is duplicate"""

Attempt_df.groupby(['user_id', 'qid', 'time']).size().sort_values(ascending=False).head()

Attempt_df = Attempt_df.drop_duplicates(subset=['user_id', 'qid', 'time'], keep='first')

Attempt_df.groupby(['user_id', 'qid', 'time']).size().sort_values(ascending=False).head()

Attempt_df.duplicated().sum()

"""Next, lets move to Chapter section"""

print(Chapter_df.dtypes)
print('Null values:', Chapter_df.isnull().sum())
print('Duplicate Values:', Chapter_df.duplicated().sum())

Chapter_df.head()

Chapter_df[Chapter_df['end_time'].isnull()]

Chapter_df=Chapter_df.dropna(subset=['end_time'])

Chapter_df.isnull().sum()

Chapter_df.shape

"""Almost 10% of the rows I have dropped because of nulls, as column wise there might be several duplicate sections so it won't affect the distinct chapter, section ID or start time.
I wasn't sure with what to put in place of nulls because end time parameter is too much variable and related with start time, chapter
"""

Chapter_df.groupby(['section_id']).head()

"""Data is free from nulls and completely duplicated rows.
There might be need of Time comparison analysis so will do later the formatting when it will be needed

#What are the primary key(s) of the given tables (post cleaning)
"""

Chapter_df['section_id'].nunique(), Chapter_df.shape[0]

Chapter_df[['section_id','chapter']].duplicated().sum()

Chapter_df.duplicated(subset=['section_id', 'chapter']).sum()

Attempt_df['qid'].duplicated().sum()

"""We removed duplicates using Attempt_df:['user_id', 'qid', 'time'] and it left no duplicates in combined so for Attempt_df or <b>Attempt data</b> we should use Composite Primary key for these 3 columns  
and for <b>Chapter data</b> we need to use combinations of ['section_id', 'chapter'] or we can add start time as well but since we already have all rows unique when we are using these columns both in combinations so there will be no need.

#Identify top 5 sections where accuracy percentage is high?

For acurracy we would need to calculate how many correct answers has been given out of total according to each section_id.
"""

# correct answers
correct_counts = Attempt_df[Attempt_df["question_status"] == "correct"].groupby("section_id").size()

# wrong answers
wrong_counts = Attempt_df[Attempt_df["question_status"] == "wrong"].groupby("section_id").size()

df = pd.DataFrame({"correct": correct_counts,"wrong": wrong_counts}).fillna(0).reset_index()
df["total_attempts"] = df["correct"] + df["wrong"]

df['accuracy_percent']=df['correct']/df['total_attempts']

df

Top_5_sections=df.sort_values(by='accuracy_percent', ascending=False).head(5)
print(Top_5_sections)

"""Based on Accuracy above are the 5 sections

#Identify bottom 2 learning units with low accuracy percentage in application level questions?
"""

# correct answers
correct_counts = Attempt_df[(Attempt_df["question_status"] == "correct") & (Attempt_df["Bloom taxonomy"] == "apply")].groupby("learning_unit_id").size()

# wrong answers
wrong_counts = Attempt_df[(Attempt_df["question_status"] == "wrong") & (Attempt_df["Bloom taxonomy"] == "apply")].groupby("learning_unit_id").size()

application_df = pd.DataFrame({"correct": correct_counts,"wrong": wrong_counts}).fillna(0).reset_index()
application_df["total_attempts"] = application_df["correct"] + application_df["wrong"]
application_df['accuracy_percent']=application_df['correct']/application_df['total_attempts']
print(application_df)

Bottom_2_learning_units=application_df.sort_values(by='accuracy_percent', ascending=True).head(2)
print(Bottom_2_learning_units)

"""Bottom two learning units in application level questions

#What percentage of questions are attempted before chapter was ended?
"""

Merged_df=pd.merge(Attempt_df, Chapter_df, on= 'section_id')

Merged_df.head(2)

Attempt_df["datetime"] = pd.to_datetime(Attempt_df["date"].astype(str) + " " + Attempt_df["time"].astype(str))

Attempt_df.head(2)

merged_df=pd.merge(Attempt_df, Chapter_df, on= 'section_id')

merged_df.head(2)

attempts_before_end = merged_df[merged_df["datetime"] < pd.to_datetime(merged_df["end_time"])]

total_attempts = len(merged_df)
attempts_before = len(attempts_before_end)

percentage_before_end = (attempts_before / total_attempts)*100

print(f"Percentage of questions attempted before chapter was ended: {percentage_before_end:.2f}%")

"""#Visualize how the attempt behaviour is spread across time of the day?"""

Attempt_df["hour"] = pd.to_datetime(Attempt_df["time"].astype(str)).dt.hour

plt.figure(figsize=(10, 5))
sns.histplot(Attempt_df["hour"], bins=24, kde=True, color="skyblue", edgecolor="black")
plt.title("Attempt Behavior Across Time of the Day")
plt.xlabel("Hour of the Day (0–23)")
plt.ylabel("Number of Attempts")
plt.xticks(range(0, 24))
plt.show()

"""#Share insights based on your analysis?

<b>Attempt Timing Behavior</b>
Most attempts are clustered during 14th and 15th hours of the day.
Histogram showed a peak around afternoon, it suggests users are most active during or after school hours.

<b>Accuracy by Section</b>
Calculated accuracy % for each section.
The top 5 sections have significantly higher accuracy, but that analysis only revealed students attempting less questions with 100% accuracy

* Otherwise insights might suggest:
Easier question sets
Stronger student understanding in those sections
Lower-performing sections could indicate areas that need review or teaching reinforcement.

<b>Accuracy by Learning Unit (Application-Level)</b>
The bottom 2 learning units had the lowest accuracy, signaling:
Possible confusion in applying concepts practically
Need for revision sessions or support materials on those specific units

<b>Attempts Before Chapter Ended</b> Around 41 percentage here means below average number of students are attempting questions on time — aligned with the learning schedule. Therefore, demands more practice sessions targeting attempt time, so that students may cope up with chapters completion time.
"""

